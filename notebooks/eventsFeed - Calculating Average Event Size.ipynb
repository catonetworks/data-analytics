{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69ab841a-84b7-4d72-baa2-8ddf3eab2ac0",
   "metadata": {},
   "source": [
    "## eventsFeed - measuring average event size\n",
    "\n",
    "The [eventsFeed](https://api.catonetworks.com/documentation/#query-eventsFeed) API query is the number one Cato Networks API call no matter how you measure it - in terms of the number of customers who call it, number of calls each week and volume of data transferred, at time of writing (February 2025) it is in pole position on every leader board. Customers use it to transfer their security, connectivity and system events in JSON format from the Cato data lake to their external SIEM vendor either using an integration [created by the vendor](https://support.catonetworks.com/hc/en-us/articles/13975273800733-Third-Party-Supported-Integrations-for-Cato-Data) or creating their own custom integration (often in conjunction with a Cato partner or other third party expertise).\n",
    "\n",
    "As well as providing the API as a **pull** method, Cato can **push** events to cloud storage, such as Amazon S3 and Azure Storage. In both cases, one frequently asked question is \"How much data am I going to consume?\". In this example, we will show how to use the **eventsFeed** query to calculate the average event size for a Cato account. This relies on the following assumptions and pre-requisites:\n",
    "* Event integration has been enabled, to start feeding events into the queue (CMA / Resources / Event Integrations / Enable integration with Cato events).\n",
    "* The account is fully onboarded with all features (especially security policies) enabled. Even if this is not the case it is still possible to obtain a reasonably close estimate of the eventual average.\n",
    "*  An API key has been created. Even if the use case is the push to cloud storage, which does not use the API, the **eventsFeed** API query can still provide a very good estimate of the average event size for pushed events.\n",
    "*  This example provides the average event size for the full event feed including all event types and subtypes. If the intention is to fetch only a subset of these by supplying a filter with the API query, this can significantly alter the average event size. In order to calculate the average in this case, the eventsFeed query used in this example would need to include the same filters.\n",
    "\n",
    "### Factors affecting the average event size\n",
    "\n",
    "Each event is a set of fields in JSON format. When measured across all Cato customers, the average event size is normally distributed but it is a wide distribution with considerable variability between the upper and lower extremes, so while it is possible to state an average event size for the entire customer base, many customers will experience an average value significantly above or below the company-wide average. This is due to:\n",
    "* Differences in policy logging configuration. For most customers, ~95% of events are Internet Firewall and WAN Firewall, so a customer who chooses not to enable those features or not to log their traffic will have a significantly different feed.\n",
    "* There is a core set of fields which all events have (such as the ID and timestamp) but otherwise, the number and content of fields varies widely between different event subtypes, so customers who disable core features or enable additional features will have a different set of events contributing to the average.\n",
    "* Differences in feature configuration. A customer who enables identity features such as SSO and Identity Agent will have richer fields including additional user identity information, which will increase the size of some of their events.\n",
    "* Other customer-specific differences, such as internal domain names. A customer whose internal namespace is \"example.local\" will have shorter device name fields than a customer with \"examplesite.examplesubdomain.country.example.local\".\n",
    "\n",
    "### How eventsFeed works\n",
    "\n",
    "[eventsFeed](https://support.catonetworks.com/hc/en-us/articles/360019839477-Cato-API-EventsFeed-Large-Scale-Event-Monitoring) works from a queue, not a timeframe. Some customers find this to be counter-intuitive and difficult to understand at first, but the reasons for doing it this way are clear - Cato is a globally distributed platform with customers who can generate well in excess of a thousand events per second. Gathering these events and processing them for distribution takes a non-zero amount of time, so it becomes extremely difficult, to guarantee event delivery based on timeframes. Using a queue ensures that, provided customers keep fetching, they are guaranteed to receive every event. A marker is used to maintain position in the queue. Submitting a request without a marker starts fetching from the beginning of the queue. At time of writing, events are aged out of the queue after three days, and each API request fetches a maximum of 3000 events.\n",
    "\n",
    "### Calculating the average event size\n",
    "\n",
    "In order to calculate the average event size, we will:\n",
    "1. Make an initial eventsFeed request with an empty marker, to start at the beginning of the queue.\n",
    "2. Process the response, adding the size of each retrieved event to a list.\n",
    "3. Make another request using the marker received from the previous request.\n",
    "\n",
    "We will do the above until we have fetched a specified number of events. At each iteration we will print the average size of the received batch of events, as well as the overall average, to show that the average converges toward a stable value as the number of samples increases.\n",
    "\n",
    "### Initialising the connection to the API.\n",
    "Firstly, let's import the libraries we need and set up the connection to the API. As usual, we assuming that the account ID and API key are preloaded as environment variables and we use our helper module to encapsulate the business of making an API call (see the [Getting Started](Getting%20Started.ipynb) notebook if any of this is unclear):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56007c0f-4385-4b4a-8248-796cf2fc4ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Initialise the API connection\n",
    "#\n",
    "import datetime\n",
    "import json\n",
    "import os\n",
    "from cato import API\n",
    "C = API(os.environ[\"CATO_API_KEY\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77174220-a5b5-496a-972b-22d7e5a048bf",
   "metadata": {},
   "source": [
    "### The eventsFeed query\n",
    "\n",
    "The eventsFeed query is documented here: [https://api.catonetworks.com/documentation/#query-eventsFeed](https://api.catonetworks.com/documentation/#query-eventsFeed). This is the query we will send:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1b6aa3d-f809-4cc9-ae99-2d30329e7b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Define the query\n",
    "#\n",
    "query = '''\n",
    "query eventsFeed($accountIDs:[ID!] $marker:String) {\n",
    "  eventsFeed(accountIDs:$accountIDs, marker:$marker) {\n",
    "    marker\n",
    "    fetchedCount\n",
    "    accounts {\n",
    "      id\n",
    "      records {\n",
    "        fieldsMap\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}'''\n",
    "\n",
    "\n",
    "#\n",
    "# Variables\n",
    "#\n",
    "variables = {\n",
    "    \"accountIDs\": [int(os.environ[\"CATO_ACCOUNT_ID\"])],\n",
    "    \"marker\": \"\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367df069-c7e0-448b-9f6b-daa3834f25ac",
   "metadata": {},
   "source": [
    "### Implement the fetch loop\n",
    "\n",
    "Let's call the query, looping around until we have reached the number of events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a98b48f-68e1-409c-a0da-d8be5bed512d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration:1 fetchedCount:3000 totalFetched:3000 batch_average:1526 total_average:1526\n",
      "iteration:2 fetchedCount:3000 totalFetched:6000 batch_average:1534 total_average:1530\n",
      "iteration:3 fetchedCount:3000 totalFetched:9000 batch_average:1564 total_average:1541\n",
      "iteration:4 fetchedCount:3000 totalFetched:12000 batch_average:1808 total_average:1608\n",
      "iteration:5 fetchedCount:3000 totalFetched:15000 batch_average:1571 total_average:1601\n",
      "iteration:6 fetchedCount:3000 totalFetched:18000 batch_average:1545 total_average:1591\n",
      "iteration:7 fetchedCount:3000 totalFetched:21000 batch_average:1537 total_average:1584\n",
      "iteration:8 fetchedCount:3000 totalFetched:24000 batch_average:1539 total_average:1578\n",
      "iteration:9 fetchedCount:3000 totalFetched:27000 batch_average:1581 total_average:1578\n",
      "iteration:10 fetchedCount:3000 totalFetched:30000 batch_average:1527 total_average:1573\n",
      "iteration:11 fetchedCount:3000 totalFetched:33000 batch_average:1558 total_average:1572\n",
      "iteration:12 fetchedCount:3000 totalFetched:36000 batch_average:1531 total_average:1568\n",
      "iteration:13 fetchedCount:3000 totalFetched:39000 batch_average:1521 total_average:1565\n",
      "iteration:14 fetchedCount:3000 totalFetched:42000 batch_average:1540 total_average:1563\n",
      "iteration:15 fetchedCount:3000 totalFetched:45000 batch_average:1521 total_average:1560\n",
      "iteration:16 fetchedCount:3000 totalFetched:48000 batch_average:1548 total_average:1559\n",
      "iteration:17 fetchedCount:3000 totalFetched:51000 batch_average:1512 total_average:1557\n",
      "iteration:18 fetchedCount:3000 totalFetched:54000 batch_average:1541 total_average:1556\n",
      "iteration:19 fetchedCount:3000 totalFetched:57000 batch_average:1531 total_average:1554\n",
      "iteration:20 fetchedCount:3000 totalFetched:60000 batch_average:1541 total_average:1554\n",
      "iteration:21 fetchedCount:3000 totalFetched:63000 batch_average:1536 total_average:1553\n",
      "iteration:22 fetchedCount:3000 totalFetched:66000 batch_average:1539 total_average:1552\n",
      "iteration:23 fetchedCount:3000 totalFetched:69000 batch_average:1563 total_average:1553\n",
      "iteration:24 fetchedCount:3000 totalFetched:72000 batch_average:1554 total_average:1553\n",
      "iteration:25 fetchedCount:3000 totalFetched:75000 batch_average:1544 total_average:1552\n",
      "iteration:26 fetchedCount:3000 totalFetched:78000 batch_average:1555 total_average:1553\n",
      "iteration:27 fetchedCount:3000 totalFetched:81000 batch_average:1549 total_average:1552\n",
      "iteration:28 fetchedCount:3000 totalFetched:84000 batch_average:1530 total_average:1552\n",
      "iteration:29 fetchedCount:3000 totalFetched:87000 batch_average:1549 total_average:1552\n",
      "iteration:30 fetchedCount:3000 totalFetched:90000 batch_average:1566 total_average:1552\n",
      "iteration:31 fetchedCount:3000 totalFetched:93000 batch_average:1545 total_average:1552\n",
      "iteration:32 fetchedCount:3000 totalFetched:96000 batch_average:1585 total_average:1553\n",
      "iteration:33 fetchedCount:3000 totalFetched:99000 batch_average:1533 total_average:1552\n",
      "iteration:34 fetchedCount:3000 totalFetched:102000 batch_average:1535 total_average:1552\n",
      "Received 102000 events >= limit:100000, stopping\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Stop when we've fetched at least this many events\n",
    "#\n",
    "limit = 100_000\n",
    "\n",
    "#\n",
    "# List to store the event sizes\n",
    "#\n",
    "event_sizes = []\n",
    "\n",
    "#\n",
    "# Fetch loop\n",
    "#\n",
    "iteration = 0\n",
    "while True:\n",
    "\n",
    "    #\n",
    "    # Make the query\n",
    "    #\n",
    "    success, result = C.send(\"eventsFeed\", variables, query)\n",
    "    iteration += 1\n",
    "\n",
    "    #\n",
    "    # Get key fields\n",
    "    #\n",
    "    fetchedCount = result[\"data\"][\"eventsFeed\"][\"fetchedCount\"]\n",
    "    marker = result[\"data\"][\"eventsFeed\"][\"marker\"]\n",
    "    records = result[\"data\"][\"eventsFeed\"][\"accounts\"][0][\"records\"]\n",
    "\n",
    "    #\n",
    "    # Process the records list. In order to get the truest estimate of\n",
    "    # event size, we convert each event back to a string with no whitespace\n",
    "    # in separators, and convert to bytes.\n",
    "    #\n",
    "    batch_sizes = []\n",
    "    for event in records:\n",
    "        event_str = json.dumps(event[\"fieldsMap\"], separators=(',',':'))\n",
    "        event_bytes = event_str.encode(\"utf-8\")\n",
    "        batch_sizes.append(len(event_bytes))\n",
    "        event_sizes.append(len(event_bytes))        \n",
    "\n",
    "    #\n",
    "    # Stop if we received nothing, to avoid divide by zero\n",
    "    #\n",
    "    if fetchedCount == 0:\n",
    "        print(f'fetchedCount:{fetchedCount}, stopping')\n",
    "        break        \n",
    "\n",
    "    #\n",
    "    # Print the current iteration's stats\n",
    "    #\n",
    "    print(f'iteration:{iteration} fetchedCount:{fetchedCount} totalFetched:{len(event_sizes)} \\\n",
    "batch_average:{int(sum(batch_sizes)/len(batch_sizes))} \\\n",
    "total_average:{int(sum(event_sizes)/len(event_sizes))}')\n",
    "\n",
    "    #\n",
    "    # Stop if we hit the limit\n",
    "    #\n",
    "    if len(event_sizes) >= limit:\n",
    "        print(f'Received {len(event_sizes)} events >= limit:{limit}, stopping')\n",
    "        break\n",
    "    \n",
    "    #\n",
    "    # Update the marker\n",
    "    #\n",
    "    variables[\"marker\"] = marker"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
